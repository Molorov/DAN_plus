model_params:
    models:
        encoder: 'FCN_Encoder'
        decoder: 'GlobalHTADecoder'

    transfer_learning:
        # model_name: [state_dict_name, checkpoint_path, learnable, strict]
        encoder: ["encoder", "../../line_OCR/ctc/outputs/DGK_199+syn_27_12/checkpoints/best_464.pt", True, True]
        decoder: ["decoder", "../../line_OCR/ctc/outputs/DGK_199+syn_27_12/checkpoints/best_464.pt", True, False]

    transfered_charset: True  # Transfer learning of the decision layer based on charset of the line HTR model
    additional_tokens: 1  # for decision layer = [<eot>, ], only for transfered charset

    input_channels: 3  # number of channels of input image
    dropout: 0.5  # dropout rate for encoder
    enc_dim: 256  # dimension of extracted features

    nb_layers: 5  # encoder
    h_max: 1000  # maximum height for encoder output (for 2D positional embedding)
    w_max: 1000  # maximum width for encoder output (for 2D positional embedding)
    l_max: 15000  # max predicted sequence (for 1D positional embedding)
    dec_num_layers: 8  # number of transformer decoder layers
    dec_num_heads: 4  # number of heads in transformer decoder layers
    dec_res_dropout: 0.1  # dropout in transformer decoder layers
    dec_pred_dropout: 0.1  # dropout rate before decision layer
    dec_att_dropout: 0.1  # dropout rate in multi head attention
    dec_dim_feedforward: 256  # number of dimension for feedforward layer in transformer decoder layers
    use_2d_pe: True  # if True, 2D positional embedding is used for values in cross-attn
    use_1d_pe: True  # use 1D positional embedding in decoder
    use_lstm: False
    attention_win: 100  # length of attention window

    save_ema: True
    # Curriculum dropout
    dropout_scheduler:
        function: 'exponential_dropout_scheduler'
        T: 50000  # 5e4

dataset_params:
    name: 'DGK_199'
    level: 'page'
    variant: '_sem'
    lan: 'bo'
    dataset_manager: 'OCRDatasetManager'
    dataset_class: 'OCRDataset'
    config:
        load_in_memory: True  # Load all images in CPU memory
        worker_per_gpu: 4  # Num of parallel processes per gpu for data loading
        width_divisor: 8  # Image width will be divided by 8
        height_divisor: 32  # Image height will be divided by 32
        padding_value: 0  # Image padding value
        padding_token:    # Label padding value
        charset_mode: "seq2seq"  # add end-of-transcription ans start-of-transcription tokens to charset
        constraints: ["add_eot", "add_sot"]  # add end-of-transcription ans start-of-transcription tokens in labels
        exclude_class: [' ']  # exclude white spaces
        normalize: True  # Normalize with mean and variance of training dataset
        preprocessings: [
            {
                type: "to_RGB",
                # if grayscaled image, produce RGB one (3 channels with same value) otherwise do nothing
            }
        ]

        synthetic_data:
            init_proba: 0.9  # begin proba to generate synthetic document
            end_proba: 0.2  # end proba to generate synthetic document
            num_steps_proba: 200000  # linearly decrease the percent of synthetic document from 90% to 20% through 200000 samples
            proba_scheduler_function: 'linear_scheduler'  # decrease proba rate linearly
            start_scheduler_at_max_line: True  # start decreasing proba only after curriculum reach max number of lines
            curriculum: True  # use curriculum learning (slowly increase number of lines per synthetic samples)
            crop_curriculum: True  # during curriculum learning, crop images under the last text line
            curr_start: 0  # start curriculum at iteration
            curr_step: 10000 # interval to increase the number of lines for curriculum learning
            min_nb_lines: 1  # initial number of lines for curriculum learning
            max_nb_lines: 15  # maximum number of lines for curriculum learning
            padding_value: 255
            font_dir: 'Kangyur'
            start_with_complex_layout: False # only applicable to Kangyur
            # config for synthetic line generation
            config:
                background_color_default: [255, 255, 255]
                background_color_eps: 15
                text_color_default:  [0, 0, 0]
                text_color_eps: 15
                font_size:
                    bo: [40, 60]
                color_mode: "RGB"
                padding_left_ratio_min: 0.00
                padding_left_ratio_max: 0.05
                padding_right_ratio_min: 0.00
                padding_right_ratio_max: 0.05
                padding_top_ratio_min: 0.02
                padding_top_ratio_max: 0.1
                padding_bottom_ratio_min: 0.02
                padding_bottom_ratio_max: 0.1
                min_len_sub_line: 4 # minumum length for sublines in two-column cjk vertical text

training_params:
    max_nb_epochs: 4000  # maximum number of epochs before to stop
    load_epoch: "last"  # ["best", "last"]: last to continue training, best to evaluate
    interval_save_weights:   # None: keep best and last only
    batch_size: 1  # mini-batch size for training
    valid_batch_size: 4  # mini-batch size for valdiation
    test_batch_size: 4
    use_ddp: False  # Use DistributedDataParallel
    ddp_port: "20027"
    use_amp: True  # Enable automatic mix-precision
    optimizers:
        all:
            class: 'Adam'
            args:
                lr: 0.0001
                amsgrad: False

    lr_schedulers:         # Learning rate schedulers
    eval_on_valid: True  # Whether to eval and logs metrics on validation set during training or not
    eval_on_valid_interval: 5  # Interval (in epochs) to evaluate during training
    focus_metric: "cer"  # Metrics to focus on to determine best epoch
    expected_metric_value: "low"  # ["high", "low"] What is best for the focus metric value

    train_metrics: ["loss_ce", "cer"]  # Metrics name for training
    eval_metrics: ["cer", 'ser']  # Metrics name for evaluation on validation set during training
    test_metrics: ['cer', 'ser', 'loer', 'map_cer']
    force_cpu: False  # True for debug purposes
    max_char_prediction: 3000  # max number of token prediction
    # Keep teacher forcing rate to 20% during whole training
    teacher_forcing_scheduler:
        min_error_rate: 0.2
        max_error_rate: 0.2
        total_num_steps: 50000 # 5e4




